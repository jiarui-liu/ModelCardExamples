# Model Card for stabilityai/stable-diffusion-xl-base-1.0

The model stabilityai/stable-diffusion-xl-base-1.0 is an improved version of the Stable Diffusion model for text-to-image synthesis, featuring a larger UNet backbone, novel conditioning techniques, and a separate diffusion-based refinement model, achieving competitive performance with black-box image generation models.

## Model Details

### Model Description

Model stabilityai/stable-diffusion-xl-base-1.0 is a latent text-to-image diffusion model (DM) called Stable Diffusion XL (SDXL). It is an improved version of Stable Diffusion, which has been widely used in various deep generative modeling tasks. SDXL leverages a two-stage pipeline, consisting of a base model and a refinement model, to generate high-quality images.

The model architecture of SDXL incorporates a convolutional UNet with a heterogeneous distribution of transformer blocks. The highest feature level does not have a transformer block, while the lower levels have 2 and 10 blocks respectively. The lowest level with 8× downsampling is removed entirely. This architecture allows for efficient computation and improved synthesis of high-frequency details in the generated images.

For text conditioning, SDXL utilizes OpenCLIP ViT-bigG and CLIP ViT-L text encoders. The model is conditioned on both the text input and the pooled text embedding from the OpenCLIP model. These conditioning mechanisms enhance the model's ability to generate coherent and relevant images based on the input text.

The training procedure of SDXL involves multiple stages. First, a base model is pretrained on an internal dataset using size and crop-conditioning for 600,000 optimization steps at a resolution of 256 × 256 pixels and a batch size of 2048. Then, a separate LDM specialized for high-quality, high-resolution data is trained in the same latent space using a noising-denoising process. During inference, latents from the base SDXL are rendered and directly diffused and denoised in the latent space using the refinement model.

The model has a total size of 2.6 billion parameters in the UNet, while the text encoders have a total size of 817 million parameters. It should be noted that the training of the model may require a minimal image size due to the two-stage architecture. The model card also mentions the possibility of improving local, high-frequency details by training a larger-batch-size autoencoder and tracking its weights.

Important disclaimers include the fact that some samples generated by the base model may exhibit low local quality, which is addressed by the refinement model. Additionally, the model card mentions the availability of more strategies and extensions for the Stable Diffusion architecture, which can be utilized to enhance the model's performance.

[More Information Needed]

- **Developed by:** Dustin Podell; Zion English; Kyle Lacey; Andreas Blattmann; Tim Dockhorn; Jonas Müller; Joe Penna; Robin Rombach
- **Funded by:** The people or organizations that fund the project of the model stabilityai/stable-diffusion-xl-base-1.0 are StabilityAI.
- **Shared by:** The model stabilityai/stable-diffusion-xl-base-1.0 is not available as a GitHub repo. [More Information Needed]
- **Model type:** The stabilityai/stable-diffusion-xl-base-1.0 model is a latent text-to-image diffusion model trained using Stable Diffusion, a deep generative modeling technique, and is designed for image generation.
- **Language(s):** The model stabilityai/stable-diffusion-xl-base-1.0 uses or processes natural human language for text conditioning and as a prompt for generating latent representations for image synthesis.
- **License:** The license being used for the model stabilityai/stable-diffusion-xl-base-1.0 is the [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0). You can find more information about the license and access the weights of the model through the following link: [SDXL-1.0 base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/).
- **Finetuned from model:** Based on the provided references, it is not specified if the model stabilityai/stable-diffusion-xl-base-1.0 is fine-tuned from another model. Therefore, the answer is "[More Information Needed]".
### Model Sources

- **Repository:** https://github.com/Stability-AI/generative-models
- **Paper:** https://arxiv.org/pdf/2307.01952.pdf
- **Demo:** The link to the demo of the model stabilityai/stable-diffusion-xl-base-1.0 is not provided in the given references. [More Information Needed]
## Uses

### Direct Use

The model stabilityai/stable-diffusion-xl-base-1.0 can be used without fine-tuning, post-processing, or plugging into a pipeline. It is a latent text-to-image diffusion model that generates images based on textual inputs. The model has undergone significant improvements compared to previous versions, resulting in superior performance.

To use the model, you can follow these steps:

1. Install the required dependencies, including PyTorch Lightning.

2. Load the model using the provided file hashes or the complete file. The model is trained using a multi-stage procedure and a discrete-time diffusion schedule.

3. Prepare your textual input. The model takes text as input and generates corresponding images.

4. Pass the text input to the model for inference. The model leverages a 3× larger UNet-backbone, additional conditioning techniques, and a diffusion-based refinement model to generate high-quality images.

Here is an example code snippet to demonstrate how to use the model:

```python
import torch
from model import StableDiffusionXL

# Load the pre-trained model
model = StableDiffusionXL.load_model("path/to/model")

# Set the device to use
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Prepare the input text
text = "A cat sitting on a mat"

# Tokenize and encode the text
input_ids = model.tokenizer.encode(text, add_special_tokens=True)
input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)

# Generate the image
with torch.no_grad():
    outputs = model.generate(input_ids)

# Save the generated image
image = outputs[0].detach().cpu().numpy()
save_image(image, "output.png")
```

Please note that this is a simplified example, and you may need to adapt it to your specific use case. For more detailed instructions and examples, you can refer to the documentation and code provided by the model developers.

[More Information Needed]

### Downstream Use

The model stabilityai/stable-diffusion-xl-base-1.0 can be used when fine-tuned for a task or when plugged into a larger ecosystem or app. Fine-tuning this model involves training it on a specific dataset for a specific task, such as image classification or image generation.

To use the model for fine-tuning, you can start by loading the pre-trained weights of stabilityai/stable-diffusion-xl-base-1.0. Then, you can train the model on your task-specific dataset, adjusting the model's parameters to optimize performance for your specific task. This fine-tuned model can then be used for various downstream tasks, such as image generation, image editing, or image classification.

Here is an example code snippet for fine-tuning the stabilityai/stable-diffusion-xl-base-1.0 model:

```python
from transformers import AutoModelForImageGeneration, AutoTokenizer
import torch

# Load the pre-trained model
model_name = "stabilityai/stable-diffusion-xl-base-1.0"
model = AutoModelForImageGeneration.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Fine-tune the model on your task-specific dataset
train_dataset = ... # Your task-specific training dataset
train_dataloader = ... # Create a dataloader for training
optimizer = ... # Choose an optimizer for training
loss_function = ... # Choose a loss function for training

for epoch in range(num_epochs):
    for batch in train_dataloader:
        inputs = tokenizer(batch["image"], return_tensors="pt", padding=True, truncation=True)
        labels = batch["label"]

        outputs = model(**inputs, labels=labels)
        loss = outputs.loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Use the fine-tuned model for inference
input_text = "Generate an image of a cat"
input_ids = tokenizer.encode(input_text, return_tensors="pt")
outputs = model.generate(input_ids)
generated_image = outputs[0]

# Process or save the generated image as needed
```

Note that the above code snippet is just an example and may need to be adapted to your specific task and dataset.

### Out-of-Scope Use

Model Card Description: stabilityai/stable-diffusion-xl-base-1.0

The stabilityai/stable-diffusion-xl-base-1.0 model is a text-to-image synthesis model that builds upon the Stable Diffusion framework. It represents a significant advancement over previous iterations of Stable Diffusion, offering improvements in synthesized image quality, prompt adherence, and composition. However, there are certain limitations and potential misuse that users should be aware of.

One foreseeable misuse of the stabilityai/stable-diffusion-xl-base-1.0 model is the generation of misleading or fake images. As with any text-to-image synthesis model, there is a risk that users may utilize the model to generate images that could potentially be used for deceptive or harmful purposes, such as creating visual content to spread misinformation or manipulate public opinion.

To address this concern, it is important to emphasize responsible use of the stabilityai/stable-diffusion-xl-base-1.0 model. Users ought not to engage in activities that involve the creation or distribution of deceptive or harmful visual content. It is crucial to respect ethical guidelines and legal regulations regarding the use of synthesized images to ensure the responsible and ethical application of this technology.

Furthermore, it is essential to note that the stabilityai/stable-diffusion-xl-base-1.0 model may encounter difficulties when rendering long, legible text. The generated text may occasionally contain random characters or exhibit inconsistencies. Overcoming this limitation requires further investigation and development of techniques to enhance the model's text generation capabilities, particularly for extended textual content.

In conclusion, while the stabilityai/stable-diffusion-xl-base-1.0 model offers significant improvements in text-to-image synthesis, it is essential to be cautious of potential misuse and ensure responsible use. Users should refrain from generating deceptive or harmful visual content and respect ethical guidelines and legal regulations. Further research and development are needed to address limitations related to rendering long, legible text.

For more information, please refer to the provided references.

### Bias, Risks, and Limitations

The known or foreseeable issues stemming from the model stabilityai/stable-diffusion-xl-base-1.0 include:

1. Increased inference cost: The model requires significant compute resources for inference, leading to higher VRAM usage and slower sampling speed.

2. Two-stage approach: The model currently uses a two-stage approach with an additional refinement model, which hampers accessibility and sampling speed. Future work should explore a single-stage approach of equal or better quality.

3. Text synthesis limitations: The model encounters difficulties when rendering long, legible text, occasionally resulting in random characters or inconsistencies in the generated text. Further investigation and development are needed to enhance its text generation capabilities, especially for extended textual content.

4. Technical improvements needed: Future work should focus on decreasing the compute requirements for inference, improving sampling speed, and exploring techniques like guidance, knowledge, and progressive distillation to enhance the model's performance.

5. Sociotechnical implications: The deployment of this model may raise concerns regarding ethical considerations, such as privacy, bias, and fairness. The model's impact on society should be carefully analyzed and mitigated to avoid any potential harms.

Please note that for a comprehensive understanding, further analysis from domain experts like ethicists, sociologists, or rights advocates is necessary. [More Information Needed]

### Recommendations

Based on the information provided in the references, the recommendations regarding the foreseeable issues for the model stabilityai/stable-diffusion-xl-base-1.0 are as follows:

1. The model is trained using the "denoiser framework" and can handle both discrete time and continuous time models. It is recommended to carefully consider the training and inference process to ensure optimal performance.

2. While the model offers improved local, high-frequency details in generated images, it comes with increased inference cost in terms of VRAM and sampling speed. Future work should focus on reducing the compute required for inference and improving sampling speed through techniques such as guidance, knowledge, and progressive distillation.

3. The model is trained in the discrete-time formulation and requires offset-noise for aesthetically pleasing results. It is important to consider the impact of this requirement and ensure proper implementation to achieve desired outcomes.

4. It is recommended to apply for access to the model through the provided links if interested in using it for research purposes. Access to both the base model and the refiner model can be granted upon approval.

Please note that further information may be required to provide more specific recommendations or address additional issues related to the model stabilityai/stable-diffusion-xl-base-1.0.

## Training Details

### Training Data

The training data for the model stabilityai/stable-diffusion-xl-base-1.0 consists of a variety of aspect ratios on images with a resolution of 1024^2. The height and width distribution of the internal dataset used for pretraining is visualized in Fig. 2 of the reference document. For more information on data pre-processing and additional filtering, please refer to the [Model Spec](https://github.com/Stability-AI/ModelSpec) documentation.

### Training Procedure

#### Preprocessing

The preprocessing steps for the data of the model stabilityai/stable-diffusion-xl-base-1.0 include the following:

1. Tokenization: The data is tokenized using byte-level tokenizers or by scaling the model to larger sizes. This helps in improving text synthesis.

2. Resizing/Rewriting: The model requires a minimal image size due to its two-stage architecture. To address this, there are two approaches. The first approach is to discard all training images below a certain minimal resolution (e.g., Stable Diffusion 1.4/1.5 discarded images below 512 pixels). The second approach is to upscale images that are smaller than the required size.

Please note that the above steps are mentioned in the references provided.

#### Training Hyperparameters

The training hyperparameters for the model stabilityai/stable-diffusion-xl-base-1.0 are not explicitly mentioned in the provided references. Therefore, [More Information Needed].

#### Speeds, Sizes, Times

The model card for stabilityai/stable-diffusion-xl-base-1.0 provides the following information:

1. Architecture: The model is based on the Stable Diffusion architecture, which is a latent diffusion model (LDM) that operates in a pretrained, learned, and fixed latent space of an autoencoder. It uses a convolutional UNet architecture with self-attention, improved upscaling layers, and cross-attention for text-to-image synthesis.

2. Transformer Blocks: The model uses a heterogeneous distribution of transformer blocks within the UNet. It omits the transformer block at the highest feature level, uses 2 and 10 blocks at the lower levels, and removes the lowest level (8× downsampling) altogether.

3. Text Conditioning: The model uses OpenCLIP ViT-bigG and CLIP ViT-L text encoders for text conditioning. It concatenates the penultimate text encoder outputs along the channel-axis and additionally conditions the model on the pooled text embedding from the OpenCLIP model.

4. Model Size: The UNet part of the model has 2.6 billion parameters, while the text encoders have a total size of 817 million parameters.

5. Training: The model is trained in the discrete-time formulation of Stable Diffusion and requires offset-noise for aesthetically pleasing results. It is trained with a batch size of 256.

Unfortunately, the model card does not provide specific information about the throughput, start or end time, checkpoint sizes, etc. for stabilityai/stable-diffusion-xl-base-1.0. [More Information Needed]

## Evaluation

### Testing Data, Factors & Metrics

#### Testing Data

The model stabilityai/stable-diffusion-xl-base-1.0 evaluates on complex, small-scale text-image datasets of natural images such as COCO [26].

#### Factors

The foreseeable characteristics that will influence how the model stabilityai/stable-diffusion-xl-base-1.0 behaves include:

1. **Domain and Context**: The model is designed for deep generative modeling of visual media, specifically for image generation. It may not perform well in other domains such as natural language or audio.

2. **Population Subgroups**: The model's performance may vary across different population subgroups. Evaluation should be disaggregated across factors such as age, gender, ethnicity, or socioeconomic status to uncover any disparities in performance. This is important to ensure fairness and avoid bias in the model's outputs.

3. **Image Size**: The model requires a minimal image size for training due to its two-stage architecture. The performance may be affected by the size of the input images. Smaller images or images below a certain resolution may result in lower quality outputs.

4. **Model Version**: The model stabilityai/stable-diffusion-xl-base-1.0 is an improved version of Stable Diffusion. It surpasses previous versions in performance. Therefore, it is important to consider the version of the model when evaluating its behavior and comparing it to other models.

Overall, evaluating the model stabilityai/stable-diffusion-xl-base-1.0 should consider the specific domain and context, disaggregate the evaluation across population subgroups, consider the image size, and take into account the model version to understand its behavior and potential disparities in performance.

#### Metrics

According to the references provided for the model stabilityai/stable-diffusion-xl-base-1.0, the metrics used for evaluation in light of tradeoffs between different errors are not explicitly mentioned. Hence, we need more information to determine the specific metrics used for evaluation.

### Results

Based on the given references, the evaluation results of the model stabilityai/stable-diffusion-xl-base-1.0 can be summarized as follows:

1. FID Scores: The FID scores for stabilityai/stable-diffusion-xl-base-1.0 are not better than the previous SD versions. In fact, the FID score for stabilityai/stable-diffusion-xl-base-1.0 is the worst among the compared models.

2. CLIP Scores: stabilityai/stable-diffusion-xl-base-1.0 shows slightly improved CLIP scores compared to previous versions. However, it is important to note that CLIP scores alone may not align with the judgement of human evaluators.

3. Prompt Adherence: In terms of prompt adherence, there is a slight preference for stabilityai/stable-diffusion-xl-base-1.0 over Midjourney.

Overall, the evaluation results suggest that stabilityai/stable-diffusion-xl-base-1.0 does not achieve better FID scores compared to previous SD versions. The CLIP scores show slight improvement, and there is a slight preference for stabilityai/stable-diffusion-xl-base-1.0 in terms of prompt adherence. However, additional quantitative performance scores and human evaluation are recommended for a comprehensive assessment.

Please note that the evaluation results provided here are based on the given references and more detailed information may be needed for a comprehensive understanding of the model's performance.

#### Summary

The evaluation results for the model stabilityai/stable-diffusion-xl-base-1.0 indicate that it does not achieve better FID scores compared to previous versions. In fact, the FID score for SDXL is the worst among the compared models. However, there is a slight improvement in CLIP-scores measured with OpenClip ViT g-14. The model's text alignment is only slightly improved compared to previous versions, and human evaluators prefer the generations of SD-XL over previous models despite the worse FID scores. The model's visual aesthetics have been questioned, and it is suggested that generative performance should be evaluated by human evaluators rather than relying solely on FID and CLIP scores. Overall, the model has shown promise in terms of prompt adherence in user studies.

## Model Examination

The model stabilityai/stable-diffusion-xl-base-1.0 is an improved version of the Stable Diffusion model for text-to-image synthesis. It achieves significant improvements in synthesized image quality, prompt adherence, and composition. However, there are still areas for further improvement:

1. Inference Cost: The model requires increased compute resources for inference, leading to higher VRAM usage and slower sampling speed. Future work aims to decrease the compute needed for inference and improve sampling speed through techniques like guidance, knowledge, and progressive distillation.

2. Two-stage Approach: Currently, the model uses a two-stage approach with an additional refinement model to generate the best samples. This hampers accessibility and sampling speed as it requires loading two large models into memory. Future work should explore ways to provide a single stage of equal or better quality.

3. Text Synthesis: While the model's scale and larger text encoder improve text rendering capabilities, there is still room for enhancement, especially for rendering long, legible text. Occasionally, the generated text may contain random characters or inconsistencies.

4. Continuous Time Formulation: The model's training in the discrete-time formulation may benefit from exploring a promising candidate, a continuous-time formulation, which allows for increased sampling flexibility and does not require noise-schedule corrections.

5. Explainability/Interpretability: This is an experimental area that developers are beginning to explore, aiming to enhance the model's explainability and interpretability.

Overall, the stabilityai/stable-diffusion-xl-base-1.0 model represents a significant advancement over previous versions of Stable Diffusion. However, there is ongoing work to improve inference cost, sampling speed, text synthesis, and further enhance the model's explainability and interpretability.

## Environmental Impact

- **Hardware Type:** The model stabilityai/stable-diffusion-xl-base-1.0 is trained on a [More Information Needed].
## Environmental Impact

- **Software Type:** The model stabilityai/stable-diffusion-xl-base-1.0 is trained using the Stable Diffusion architecture, which is a latent text-to-image diffusion model (DM). It utilizes a convolutional UNet architecture and a pre-trained text encoder, specifically OpenCLIP ViT-bigG in combination with CLIP ViT-L. The model has a total size of 2.6B parameters in the UNet. It also incorporates cross-attention layers and conditions the model on the pooled text embedding from the OpenCLIP model. The model is trained on a larger batch size (256) and is capable of generating high-frequency details in generated images. Therefore, the software type that the model is trained on is a latent text-to-image diffusion model.
- **Hours used:** Based on the references provided, the amount of time used to train the model stabilityai/stable-diffusion-xl-base-1.0 is not mentioned. Therefore, the answer is "[More Information Needed]".
- **Cloud Provider:** The model stabilityai/stable-diffusion-xl-base-1.0 is trained on an unknown cloud provider. [More Information Needed]
- **Carbon Emitted:** The amount of carbon emitted when training the model stabilityai/stable-diffusion-xl-base-1.0 is not provided in the given references. [More Information Needed]
## Technical Specification

### Model Architecture and Objective

The model stabilityai/stable-diffusion-xl-base-1.0 is an improved version of the Stable Diffusion architecture for latent text-to-image diffusion modeling. It incorporates several strategies to enhance performance:

1. Model Size: The UNet-backbone of stabilityai/stable-diffusion-xl-base-1.0 is 3 times larger compared to previous Stable Diffusion models, resulting in a more powerful model.

2. Conditioning Techniques: Two additional conditioning techniques are used in stabilityai/stable-diffusion-xl-base-1.0. It leverages OpenCLIP ViT-bigG and CLIP ViT-L text encoders for text conditioning. The model is also conditioned on the pooled text embedding from the OpenCLIP model.

3. Heterogeneous Distribution of Transformer Blocks: Unlike the original Stable Diffusion architecture, stabilityai/stable-diffusion-xl-base-1.0 employs a heterogeneous distribution of transformer blocks within the UNet. It omits the transformer block at the highest feature level, uses 2 and 10 blocks at the lower levels, and removes the lowest level with 8× downsampling altogether.

4. Separate Diffusion-based Refinement Model: stabilityai/stable-diffusion-xl-base-1.0 incorporates a separate diffusion-based refinement model to improve local, high-frequency details in generated images.

The objective of stabilityai/stable-diffusion-xl-base-1.0 is to generate high-quality images from textual descriptions using latent diffusion models. It aims to improve performance compared to previous versions of Stable Diffusion, as demonstrated by user studies.

[More Information Needed]

### Compute Infrastructure

The compute infrastructure details about the model stabilityai/stable-diffusion-xl-base-1.0 are not provided in the given references. [More Information Needed]

## Citation

```
@misc{dustin-sdxl,
    author = {Dustin Podell and
              Zion English and
              Kyle Lacey and
              Andreas Blattmann and
              Tim Dockhorn and
              Jonas Müller and
              Joe Penna and
              Robin Rombach},
    title  = {SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
    url    = {https://arxiv.org/pdf/2307.01952.pdf}
}
